from __future__ import annotations
import logging
from collections.abc import Iterable, Mapping, Sequence
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING, Any, Literal, overload
from typing_extensions import override

from pynisher import MemoryLimitException, TimeoutException
from smac import HyperparameterOptimizationFacade, Scenario
from smac.runhistory import (
    StatusType,
    TrialInfo as SMACTrialInfo,
    TrialValue as SMACTrialValue,
)

from amltk.optimization import Metric, Optimizer, Trial
from amltk.pipeline import Node
from amltk.randomness import as_int
from amltk.store import PathBucket

if TYPE_CHECKING:
    from typing_extensions import Self

    from ConfigSpace import ConfigurationSpace
    from smac.facade import AbstractFacade

    from amltk.types import FidT, Seed

from smac.model.random_forest.random_forest import RandomForest

from smac.initial_design.sobol_design import SobolInitialDesign
from smac.intensifier.intensifier import Intensifier

from ConfigSpace import ConfigurationSpace
import types

from optimizers.SMAC_utils.SMAC_utils import (
    FixedSet,
    select_configurations,
    FixedSetRandomInitialDesign,
)
from smac.random_design.probability_design import ProbabilityRandomDesign
from smac.main.config_selector import ConfigSelector
from amltk.profiling import Profile

logger = logging.getLogger(__name__)
#logging.basicConfig(filename='example.log', encoding='utf-8', level=logging.DEBUG)

class SMACOptimizer(Optimizer[SMACTrialInfo]):
    """An optimizer that uses SMAC to optimize a config space."""

    def __init__(
        self,
        *,
        facade: AbstractFacade,
        bucket: PathBucket | None = None,
        metrics: Metric | Sequence[Metric],
        fidelities: Mapping[str, FidT] | None = None,
        time_profile: str | None = None,
    ) -> None:
        """Initialize the optimizer.

        Args:
            facade: The SMAC facade to use.
            bucket: The bucket given to trials generated by this optimizer.
            metrics: The metrics to optimize.
            fidelities: The fidelities to use, if any.
            time_profile: The profile to use to get time information to the
                optimizer. Must use `trial.profile(time_profile)` in your
                target function then.
        """
        # We need to very that the scenario is correct incase user pass in
        # their own facade construction
        assert list(self.crash_costs(metrics).values()) == facade.scenario.crash_cost

        metrics = metrics if isinstance(metrics, Sequence) else [metrics]
        super().__init__(metrics=metrics, bucket=bucket)
        self.facade = facade
        self.fidelities = fidelities
        self.time_profile = time_profile

    @classmethod
    def create(
        cls,
        *,
        space: ConfigurationSpace | Node,
        metrics: Metric | Sequence[Metric],
        bucket: PathBucket | str | Path | None = None,
        time_profile: str | None = None,
        deterministic: bool = True,
        seed: Seed | None = None,
        fidelities: Mapping[str, FidT] | None = None,
        continue_from_last_run: bool = False,
        logging_level: int | Path | Literal[False] | None = False,
        n_configs=None,
        n_trials=100,
        initial_configs=None,
        limit_to_configs=None,
    ) -> Self:
        seed = as_int(seed)
        match bucket:
            case None:
                bucket = PathBucket(
                    f"{cls.__name__}-{datetime.now().isoformat()}",
                )
            case str() | Path():
                bucket = PathBucket(bucket)
            case bucket:
                bucket = bucket  # noqa: PLW0127

        # NOTE SMAC always minimizes! Hence we make it a minimization problem
        metric_names: str | list[str]
        if isinstance(metrics, Sequence):
            metric_names = [metric.name for metric in metrics]
        else:
            metric_names = metrics.name

        if isinstance(space, Node):
            space = space.search_space(parser=cls.preferred_parser())
        facade_cls: type[AbstractFacade]
        if fidelities:
            if len(fidelities) == 1:
                v = next(iter(fidelities.values()))
                min_budget, max_budget = v
            else:
                min_budget, max_budget = 1.0, 100.0


        scenario = Scenario(
            objectives=metric_names,
            configspace=space,
            output_directory=bucket.path / "smac3_output",
            seed=seed,
            deterministic=deterministic,
            n_trials=n_trials,
            crash_cost=list(cls.crash_costs(metrics).values()),
        )
        facade_cls = HyperparameterOptimizationFacade
        if limit_to_configs is not None:
            model = RandomForest(
                log_y=True,
                n_trees=10,
                bootstrapping=True,
                ratio_features=1.0,
                min_samples_split=2,
                min_samples_leaf=1,
                max_depth=2**20,
                configspace=scenario.configspace,
                instance_features=scenario.instance_features,
                seed=scenario.seed,
            )

            from smac.acquisition.function.expected_improvement import EI

            acquisition_function = EI(log=True)
            acquisition_function._model = model
            acquisition_function.model = model

            initial_design = FixedSetRandomInitialDesign(
                limit_to_configs,
                scenario=scenario,
                n_configs=n_configs,
                n_configs_per_hyperparameter=10,
                max_ratio=0.25,
                additional_configs=initial_configs,
            )
            initial_design.select_configurations = types.MethodType(
                select_configurations, initial_design
            )

            acquisition_maximizer = FixedSet(
                configspace=scenario.configspace,
                configurations=limit_to_configs,
                acquisition_function=acquisition_function,
                seed=seed,
            )
            random_design = ProbabilityRandomDesign(probability=0.2)

            config_selector = ConfigSelector(
                scenario, retrain_after=1, retries=16 + len(limit_to_configs)
            )


        else:
            model = None
            acquisition_maximizer = None
            acquisition_function = None
            random_design = None
            config_selector = ConfigSelector(
                scenario, retrain_after=1, retries=16 + 200
            )
            initial_design = SobolInitialDesign(
                scenario=scenario,
                n_configs=n_configs,
                n_configs_per_hyperparameter=10,
                max_ratio=0.25,
                additional_configs=initial_configs,
            )
            initial_design.select_configurations = types.MethodType(
                select_configurations, initial_design
            )

        # there is a bug with warm start (resumming) 
        intensifier = Intensifier(
                scenario=scenario,
                max_config_calls=3,
                max_incumbents=10,
                retries=  16+ 200,
            )
        facade = facade_cls(
            scenario=scenario,
            target_function="dummy",  # NOTE: https://github.com/automl/SMAC3/issues/946
            overwrite=True,
            logging_level=logging_level,
            multi_objective_algorithm=facade_cls.get_multi_objective_algorithm(
                scenario=scenario,
            ),
            acquisition_maximizer=acquisition_maximizer,
            acquisition_function=acquisition_function,
            model=model,
            random_design=random_design,
            initial_design=initial_design,
            config_selector=config_selector,
            intensifier = intensifier
        )
        return cls(facade=facade, fidelities=fidelities, bucket=bucket, metrics=metrics)

    @overload
    def ask(self, n: int) -> Iterable[Trial[SMACTrialInfo]]: ...

    @overload
    def ask(self, n: None = None) -> Trial[SMACTrialInfo]: ...

    @override
    def ask(
        self,
        n: int | None = None,
    ) -> Trial[SMACTrialInfo] | Iterable[Trial[SMACTrialInfo]]:
        """Ask the optimizer for a new config.

        Args:
            n: The number of configs to ask for. If `None`, ask for a single config.


        Returns:
            The trial info for the new config.
        """
        if n is not None:
            return (self.ask(n=None) for _ in range(n))

        smac_trial_info = self.facade.ask()
        config = smac_trial_info.config
        budget = smac_trial_info.budget
        instance = smac_trial_info.instance
        seed = smac_trial_info.seed

        if self.fidelities and budget:
            if len(self.fidelities) == 1:
                k, _ = next(iter(self.fidelities.items()))
                trial_fids = {k: budget}
            else:
                trial_fids = {"budget": budget}
        else:
            trial_fids = None

        config_id = self.facade.runhistory.config_ids[config]
        unique_name = f"{config_id=}_{seed=}_{budget=}_{instance=}"
        trial: Trial[SMACTrialInfo] = Trial.create(
            name=unique_name,
            config=dict(config),
            info=smac_trial_info,
            seed=seed,
            fidelities=trial_fids,
            bucket=self.bucket / unique_name,
            metrics=self.metrics,
        )
        logger.debug(f"Asked for trial {trial.name}")
        return trial


    def tell(self, report: Trial.Report[SMACTrialInfo]) -> None:
        """Tell the optimizer the result of the sampled config.

        Args:
            report: The report of the trial.
        """
        assert report.trial.info is not None

        costs: dict[str, float] = {}
        for name, metric in self.metrics.items():
            value = report.values.get(metric.name)
            if value is None:
                if report.status == Trial.Status.SUCCESS:
                    raise ValueError(
                        f"Could not find metric '{metric.name}' in report values."
                        " Make sure you use `trial.success()` in your target function."
                        " So that we can report the metric value to SMAC.",
                    )
                value = metric.worst

            costs[name] = metric.normalized_loss(value)

        logger.debug(f"Reporting for trial {report.trial.name} with costs: {costs}")

        cost = next(iter(costs.values())) if len(costs) == 1 else list(costs.values())

        # If we're successful, get the cost and times and report them
        params: dict[str, Any]
        match report.status:
            case Trial.Status.SUCCESS | Trial.Status.FAIL:
                smac_status = (
                    StatusType.SUCCESS
                    if report.status == Trial.Status.SUCCESS
                    else StatusType.CRASHED
                )
                params = {"cost": cost, "status": smac_status}
            case Trial.Status.CRASHED | Trial.Status.UNKNOWN:
                params = {"cost": cost, "status": StatusType.CRASHED}

        if self.time_profile:
            profile = report.trial.profiles.get(self.time_profile)
            match profile:
                # If it was a success, we kind of expect there to have been this
                # timing. Otherwise, for failure we don't necessarily expect it.
                case None if report.status in Trial.Status.SUCCESS:
                    raise ValueError(
                        f"Could not find profile '{self.time_profile}' in trial"
                        " as specified by `time_profile` during construction."
                        " Make sure you use `with trial.profile(time_profile):`"
                        " in your target function. So that we can report the"
                        " timing information to SMAC.",
                    )
                case Profile.Interval(time=timer):
                    params.update(
                        {
                            "time": timer.duration,
                            "starttime": timer.start,
                            "endtime": timer.end,
                        },
                    )
                case None:
                    pass

        match report.exception:
            case None:
                pass
            case MemoryLimitException():
                params["status"] = StatusType.MEMORYOUT
                params["additional_info"] = {
                    "exception": str(report.exception),
                    "traceback": report.traceback,
                }
            case TimeoutException():
                params["status"] = StatusType.TIMEOUT
                params["additional_info"] = {
                    "exception": str(report.exception),
                    "traceback": report.traceback,
                }
            case _:
                params["additional_info"] = {
                    "exception": str(report.exception),
                    "traceback": report.traceback,
                }

        self.facade.tell(report.trial.info, value=SMACTrialValue(**params), save=True)

    @override
    @classmethod
    def preferred_parser(cls) -> Literal["configspace"]:
        """The preferred parser for this optimizer."""
        return "configspace"

    @classmethod
    def crash_costs(cls, metric: Metric | Iterable[Metric]) -> dict[str, float]:
        """Get the crash cost for a metric for SMAC."""
        match metric:
            case Metric():
                return {metric.name: metric.normalized_loss(metric.worst)}
            case Iterable():
                return {
                    metric.name: metric.normalized_loss(metric.worst)
                    for metric in metric
                }
            case _:
                raise TypeError(
                    f"Expected a Metric, Mapping, or Iterable of Metrics. Got {metric}",
                )